{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😊\\n\\nThe length of a day on Earth is approximately 24 hours. 🕰️\\n\\nIn Korean:\\n\\n 지구의 자전 주기는 약 24시간입니다.\\n\\n(Note: \"자전\" means \"rotation\" or \"revolution\", and \"주기\" means \"period\" or \"cycle\".)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# model\n",
    "llm = Ollama(model='llama3')\n",
    "\n",
    "# chain execute\n",
    "\n",
    "llm.invoke(\"지구의 자전 주기는?, 한국어로 번역해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a world-class technical documentation writer, I'm excited to share how Langsmith can assist with testing.\n",
      "\n",
      "Langsmith's AI-powered writing capabilities can significantly enhance the testing process by generating test cases, test data, and even testing scripts. Here are some ways Langsmith can help:\n",
      "\n",
      "1. **Test Case Generation**: Langsmith can analyze requirements and generate test cases based on the functionality, ensuring comprehensive coverage.\n",
      "2. **Test Data Creation**: The AI can create realistic test data for different scenarios, reducing the need to manually generate data and increasing test efficiency.\n",
      "3. **Testing Scripts**: Langsmith can assist in generating testing scripts, including step-by-step instructions for manual or automated testing.\n",
      "4. **Defect Reporting**: When defects are found during testing, Langsmith can help generate clear, concise reports outlining the issue, steps to reproduce it, and expected results.\n",
      "5. **Test Summary Reports**: The AI can summarize test results, providing a high-level overview of passed and failed tests, making it easier to identify areas for improvement.\n",
      "\n",
      "By leveraging Langsmith's capabilities, you can:\n",
      "\n",
      "* Reduce testing time and effort\n",
      "* Improve test quality and coverage\n",
      "* Enhance collaboration among team members\n",
      "* Ensure more accurate and consistent reporting\n",
      "\n",
      "In summary, Langsmith is an invaluable tool for testing, enabling the generation of high-quality test cases, data, scripts, reports, and summaries. This allows your team to focus on the bigger picture – delivering exceptional products or services!\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'input': \"how can langsmith help with testing?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://ttoro.tistory.com/89\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"주어진 내용을 오직 기반으로 질문에 대답해줘:\n",
    "                                          \n",
    "                                          <context>\n",
    "                                          {context}\n",
    "                                          <context>\n",
    "                                          \n",
    "                                          Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context, it seems that the problem is related to image classification or object detection.\n",
      "\n",
      "Answer: We approached the problem using a classification model (CNN) and YOLOv5. 🤔\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "resp = document_chain.invoke({\n",
    "    \"input\": \"벽지하자 문제를 어떻게 접근했나요?\",\n",
    "    \"context\": [Document(page_content=\"classification model인 CNN과 yolov5를 통해 문제를 해결했습니다.\")]\n",
    "})\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the given content, the approach taken for the wall crack problem is as follows:\n",
      "\n",
      "* Initially, a classification-based approach was attempted, using various pre-trained CNN models (EfficientNet, ResNet, VGG) and image augmentation techniques to prevent overfitting.\n",
      "* However, due to the high degree of variation in the images and the presence of noise and gaps in the wall cracks, the accuracy remained stuck at around 50%.\n",
      "* To overcome this challenge, a segmentation-based approach was adopted, focusing on detecting specific features within the images rather than classifying the entire image.\n",
      "* The team used Yolo models (yolo_v5s, v5m, v5l, v8s, v8m, v8l) and trained them using labeled data from approximately 3,000 images, with each team member manually labeling around 600 images.\n",
      "* Although the accuracy was not high, the model was able to detect some of the wall cracks reasonably well.\n",
      "* The team recognized that this approach had limitations and that a more accurate solution would require further development, possibly using a 2-stage approach or multi-modal models.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"벽지하자 문제를 어떻게 접근했나요?\"})\n",
    "print(response['answer'])\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"벽지하자 분석을 어떻게 했는지 알겠어?\"), AIMessage(content=\"네!\")]\n",
    "result = retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"어떻게 했는지 알려줘\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"벽지하자분석을 어떻게 했는지 알겠어?\"), AIMessage(content=\"Yes!\")]\n",
    "result = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history : [HumanMessage(content='벽지하자분석을 어떻게 했는지 알겠어?'), AIMessage(content='Yes!')]\n",
      "input : Tell me how\n",
      "answer : Based on the context, I'll answer your questions.\n",
      "\n",
      "**Q1: What was the goal of the AIVLE 3rd project?**\n",
      "A1: The goal was to develop an AI model that can accurately classify the types of defects in images based on a dataset provided by DACON (Dacon's Official Description).\n",
      "\n",
      "**Q2: How did you approach this problem?**\n",
      "A2: You approached it as a classification problem first, using various pre-trained CNN models and image augmentation techniques. However, you found that this approach didn't improve the score much beyond 50%. Therefore, you shifted to a segmentation approach, focusing on detecting defects in images.\n",
      "\n",
      "**Q3: What were some of the challenges you faced during the project?**\n",
      "A3: One major challenge was the difficulty in labeling the data due to the variability in image features. You also faced issues with overfitting and underfitting, as well as the need for more precise labeling data.\n",
      "\n",
      "**Q4: How did you resolve these challenges?**\n",
      "A4: To address these challenges, you used a YOLO (You Only Look Once) model and experimented with different sizes of models. You also tried to refine your labeling process by reviewing the data and identifying ambiguous cases.\n",
      "\n",
      "Let me know if you have any further questions! 😊\n"
     ]
    }
   ],
   "source": [
    "for key, value in result.items():\n",
    "\n",
    "    if key == 'context':\n",
    "        pass\n",
    "    else:\n",
    "        print(f'{key} : {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
